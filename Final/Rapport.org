#+TITLE: 
#+LANGUAGE:  fr
#+OPTIONS: H:5 author:nil email:nil creator:nil timestamp:nil skip:nil toc:nil ^:nil
#+TAGS: Arnaud(a) Luka(l)
#+TAGS: noexport(n) deprecated(d)
#+EXPORT_SELECT_TAGS: export
#+EXPORT_EXCLUDE_TAGS: noexport
#+LATEX_CLASS: svjour3
# #+LaTeX_CLASS: article
# #+LaTeX_CLASS: acm-proc-article-sp
#+BABEL: :session *R* :cache yes :results output graphics :exports both :tangle yes 
#+LATEX_HEADER: \usepackage[T1]{fontenc}
#+LATEX_HEADER: \usepackage[utf8]{inputenc}
#+LATEX_HEADER: %\usepackage{fixltx2e}
#+LATEX_HEADER: \usepackage{ifthen,figlatex}
#+LATEX_HEADER: \usepackage{longtable}
#+LATEX_HEADER: \usepackage{float}
#+LATEX_HEADER: \usepackage{wrapfig}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage[export]{adjustbox}
#+LATEX_HEADER: \usepackage{xspace}
#+LATEX_HEADER: \usepackage{amsmath,amssymb}
#+LATEX_HEADER: \usepackage[french]{babel}
#+LATEX_HEADER: \AtBeginDocument{
#+LATEX_HEADER:   \definecolor{pdfurlcolor}{rgb}{0,0,0.6}
#+LATEX_HEADER:   \definecolor{pdfcitecolor}{rgb}{0,0.6,0}
#+LATEX_HEADER:   \definecolor{pdflinkcolor}{rgb}{0.6,0,0}
#+LATEX_HEADER:   \definecolor{light}{gray}{.85}
#+LATEX_HEADER:   \definecolor{vlight}{gray}{.95}
#+LATEX_HEADER: }
#+LATEX_HEADER: %\usepackage[paper=letterpaper,margin=1.61in]{geometry}
#+LATEX_HEADER: \usepackage{url} \urlstyle{sf}
#+LATEX_HEADER: \usepackage[normalem]{ulem}
#+LATEX_HEADER: \usepackage{todonotes}
#+LATEX_HEADER: \usepackage[colorlinks=true,citecolor=pdfcitecolor,urlcolor=pdfurlcolor,linkcolor=pdflinkcolor,pdfborder={0 0 0}]{hyperref}
#+LATEX_HEADER: \usepackage[round-precision=3,round-mode=figures,scientific-notation=true]{siunitx}

#+LaTeX_HEADER: % \usepackage{minted}
#+LaTeX_HEADER: % \usepackage{verbments}
#+LATEX_HEADER: % \usepackage{verbatim}
#+LATEX_HEADER: % \usepackage{alltt}

#+BEGIN_LaTeX
\newcommand{\AL}[2][inline]{\todo[color=green!50,#1]{\sf \textbf{AL:} #2}\xspace}
\newcommand{\LS}[2][inline]{\todo[color=green!50,#1]{\sf \textbf{LS:} #2}\xspace}

\let\oldcite=\cite
\renewcommand\cite[2][]{~\ifthenelse{\equal{#1}{}}{\oldcite{#2}}{\oldcite[#1]{#2}}\xspace}
\let\oldref=\ref
\def\ref#1{~\oldref{#1}\xspace}
\def\ie{i.e.,\xspace}
\def\eg{e.g.,\xspace}
\def\qrmspu{\texttt{QRM\_StarPU}\xspace}
\sloppy
#+END_LaTeX

#+BEGIN_LaTeX  
\title{Simulation d'applications dynamiques pour plateformes de
calculs hautes performances%\thanks{Grants or other notes
%about the article that should go on the front page should be
%placed here. General acknowledgments should be placed at the end of the article.}
}
%\subtitle{Do you have a subtitle?\\ If so, write it here}

%\titlerunning{StarPU SMPI}        % if too long for running head

\author{Steven QUINITO MASNADA  \\ \\
        Encadrants : Arnaud LEGRAND \and Luka STANISIC  %if many names separate them with \and.
}

%\authorrunning{Steven QUINITO MASNADA} % if too long for running head

\institute{%F. Author \at
           %   first address \\
           %   Tel.: +123-45-678910\\
           %   Fax: +123-45-678910\\
           %   \email{fauthor@example.com}           %  \\
%             \emph{Present address:} of F. Author  %  if needed
           %\and
           %S. Author \at
           %   second address
}

\date{Juin 2015}
% The correct dates will be entered by the editor

\maketitle

#+END_LaTeX


#+BEGIN_abstract
  Actuellement, la majorité des supercalcultateurs sont des noeuds
  composés de machines hybrides. Pour tirer partie de toute la
  puissance de calcul disponible, il est indispensable d'avoir un
  programme qui soit dynamique. Cependant, les APIs de programmation
  classiques conduisent à une mise en oeuvre très complexe.
  Le paradigme de programmation par tâches couplé à un système
  dynamique permet de répondre à ce problème, mais il est difficile
  d'en évaluer les performances. L'objectif de notre étude est donc de
  mettre en place les dispositifs nécessaire à l'évaluation de
  performances du système. 
  \newpage
#+END_abstract

* Questions:							   :noexport:
    - Which conference?
      - General conference ?
      - Possibly IPDPS, but it is only in October

    - Which journal: JPDC, ParCo, TPDS ?
* Extracting traces from data files				   :noexport:
  For fourmi machine:
#+begin_src sh :results output :exports none
mkdir -p tmp
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou3/SoloStarpuData0.org tmp/native_fourmi_tp6
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou3/SimgridStarpuData0.org tmp/simgrid_fourmi_tp6
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou3/SoloStarpuData1.org tmp/native_fourmi_karted
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou3/SimgridStarpuData1.org tmp/simgrid_fourmi_karted
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou3/SoloStarpuData2.org tmp/native_fourmi_EternityII_E
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou3/SimgridStarpuData2.org tmp/simgrid_fourmi_EternityII_E
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou3/SoloStarpuData3.org tmp/native_fourmi_degme
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou3/SimgridStarpuData3.org tmp/simgrid_fourmi_degme
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou3/SoloStarpuData4.org tmp/native_fourmi_cat_ears_4_4
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou3/SimgridStarpuData4.org tmp/simgrid_fourmi_cat_ears_4_4
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou3/SoloStarpuData5.org tmp/native_fourmi_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou3/SimgridStarpuData5.org tmp/simgrid_fourmi_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou3/SoloStarpuData6.org tmp/native_fourmi_hirlam
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou3/SimgridStarpuData6.org tmp/simgrid_fourmi_hirlam
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou3/SoloStarpuData7.org tmp/native_fourmi_TF16
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou3/SimgridStarpuData7.org tmp/simgrid_fourmi_TF16
#+end_src

#+RESULTS:


  For riri machine with 10 CPUs:
#+begin_src sh :results output :exports none
mkdir -p tmp
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SoloStarpuData0.org tmp/native_riri10_tp6
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData0.org tmp/simgrid_riri10_tp6
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SoloStarpuData1.org tmp/native_riri10_karted
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData1.org tmp/simgrid_riri10_karted
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SoloStarpuData2.org tmp/native_riri10_EternityII_E
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData2.org tmp/simgrid_riri10_EternityII_E
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SoloStarpuData3.org tmp/native_riri10_degme
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData3.org tmp/simgrid_riri10_degme
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SoloStarpuData4.org tmp/native_riri10_cat_ears_4_4
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData4.org tmp/simgrid_riri10_cat_ears_4_4
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SoloStarpuData5.org tmp/native_riri10_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData5.org tmp/simgrid_riri10_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SoloStarpuData6.org tmp/native_riri10_hirlam
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData6.org tmp/simgrid_riri10_hirlam
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SoloStarpuData7.org tmp/native_riri10_TF16
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData7.org tmp/simgrid_riri10_TF16
#+end_src

#+RESULTS:

  For riri machine with 40 CPUs:
#+begin_src sh :results output :exports none
mkdir -p tmp
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SoloStarpuData8.org tmp/native_riri40_tp6
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData8.org tmp/simgrid_riri40_tp6
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SoloStarpuData9.org tmp/native_riri40_karted
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData9.org tmp/simgrid_riri40_karted
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SoloStarpuData10.org tmp/native_riri40_EternityII_E
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData10.org tmp/simgrid_riri40_EternityII_E
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SoloStarpuData11.org tmp/native_riri40_degme
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData11.org tmp/simgrid_riri40_degme
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SoloStarpuData12.org tmp/native_riri40_cat_ears_4_4
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData12.org tmp/simgrid_riri40_cat_ears_4_4
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SoloStarpuData13.org tmp/native_riri40_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData13.org tmp/simgrid_riri40_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SoloStarpuData14.org tmp/native_riri40_hirlam
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData14.org tmp/simgrid_riri40_hirlam
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SoloStarpuData15.org tmp/native_riri40_TF16
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData15.org tmp/simgrid_riri40_TF16
#+end_src

#+RESULTS:

  For extrapolated riri machine with 100 and 400 CPUs:
#+begin_src sh :results output :exports none
mkdir -p tmp
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData16.org tmp/simgrid_riri100_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataTou4/SimgridStarpuData17.org tmp/simgrid_riri400_e18
#+end_src

#+RESULTS:

  Extracting makespan for all traces:
#+begin_src sh :shebang "#!/bin/bash" :results output :exports none
output="tmp/makespans.out"
matrices=(tp-6 karted EternityII_E degme cat_ears_4_4 e18 hirlam TF16)
echo "Matrix, Nthreads, Native Time [ms], SimGrid Time [ms], Diff Time" > $output

i=0
#matrices=(tp-6 karted EternityII_E degme cat_ears_4_4 e18 hirlam TF16 cat_ears_4_4_ownmodel)
nthreads=8
datafolder="starpu-simgrid/data/dataTou3"
for matrix in ${matrices[@]}
do
   native_time=$(tail -1 $datafolder/SoloStarpuData$i.org)
   simgrid_time=$(tail -1 $datafolder/SimgridStarpuData$i.org)
   diff_time=$(bc -l <<< "(1 - ($simgrid_time / $native_time)) * 100" | sed 's/\(-\?[0-9]*\.[0-9]\?\)[0-9]*/\1/')
   echo "$matrix, $nthreads,  $native_time, $simgrid_time, $diff_time" >> $output
   i=`expr $i + 1`
done

i=0
nthreads=10
datafolder="starpu-simgrid/data/dataTou4"
for matrix in ${matrices[@]}
do
   native_time=$(tail -1 $datafolder/SoloStarpuData$i.org)
   simgrid_time=$(tail -1 $datafolder/SimgridStarpuData$i.org)
   diff_time=$(bc -l <<< "(1 - ($simgrid_time / $native_time)) * 100" | sed 's/\(-\?[0-9]*\.[0-9]\?\)[0-9]*/\1/')
   echo "$matrix, $nthreads,  $native_time, $simgrid_time, $diff_time" >> $output
   i=`expr $i + 1`
done

nthreads=40
datafolder="starpu-simgrid/data/dataTou4"
for matrix in ${matrices[@]}
do
   native_time=$(tail -1 $datafolder/SoloStarpuData$i.org)
   simgrid_time=$(tail -1 $datafolder/SimgridStarpuData$i.org)
   diff_time=$(bc -l <<< "(1 - ($simgrid_time / $native_time)) * 100" | sed 's/\(-\?[0-9]*\.[0-9]\?\)[0-9]*/\1/')
   echo "$matrix, $nthreads,  $native_time, $simgrid_time, $diff_time" >> $output
   i=`expr $i + 1`
done

# For extrapolated data
simgrid_100_time=$(tail -1 $datafolder/SimgridStarpuData16.org)
echo "e18, 100,  0, $simgrid_100_time, 0" >> $output
simgrid_400_time=$(tail -1 $datafolder/SimgridStarpuData17.org)
echo "e18, 400,  0, $simgrid_400_time, 0" >> $output
#+end_src

#+RESULTS:

  Extracting traces with memory consumption
#+begin_src sh :results output :exports none
mkdir -p tmp
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataQMem/SoloStarpuData0.org tmp/native_hirlam_1_memcon
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataQMem/SoloStarpuData1.org tmp/native_hirlam_2_memcon
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataQMem/SoloStarpuData2.org tmp/native_hirlam_3_memcon
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataQMem/SimgridStarpuData0.org tmp/simgrid_hirlam_memcon
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataQMem/SoloStarpuData3.org tmp/native_e18_1_memcon
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataQMem/SoloStarpuData4.org tmp/native_e18_2_memcon
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataQMem/SoloStarpuData5.org tmp/native_e18_3_memcon
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataQMem/SimgridStarpuData1.org tmp/simgrid_e18_memcon
#+end_src

#+RESULTS:


  Extracting extrapolation data on riri machine with e18 and sls matrices:
#+begin_src sh :results output :exports none
mkdir -p tmp
# e18 matrix
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SoloStarpuData1.org tmp/native_extrapol_2_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SimgridStarpuData1.org tmp/simgrid_extrapol_2_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SoloStarpuData2.org tmp/native_extrapol_4_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SimgridStarpuData2.org tmp/simgrid_extrapol_4_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SoloStarpuData3.org tmp/native_extrapol_5_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SimgridStarpuData3.org tmp/simgrid_extrapol_5_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SoloStarpuData4.org tmp/native_extrapol_8_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SimgridStarpuData4.org tmp/simgrid_extrapol_8_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SoloStarpuData5.org tmp/native_extrapol_10_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SimgridStarpuData5.org tmp/simgrid_extrapol_10_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SoloStarpuData6.org tmp/native_extrapol_40_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SimgridStarpuData6.org tmp/simgrid_extrapol_40_e18
# sls matrix
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SoloStarpuData8.org tmp/native_extrapol_2_sls
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SimgridStarpuData8.org tmp/simgrid_extrapol_2_sls
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SoloStarpuData9.org tmp/native_extrapol_4_sls
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SimgridStarpuData9.org tmp/simgrid_extrapol_4_sls
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SoloStarpuData10.org tmp/native_extrapol_5_sls
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SimgridStarpuData10.org tmp/simgrid_extrapol_5_sls
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SoloStarpuData11.org tmp/native_extrapol_8_sls
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SimgridStarpuData11.org tmp/simgrid_extrapol_8_sls
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SoloStarpuData12.org tmp/native_extrapol_10_sls
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SimgridStarpuData12.org tmp/simgrid_extrapol_10_sls
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SoloStarpuData13.org tmp/native_extrapol_40_sls
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SimgridStarpuData13.org tmp/simgrid_extrapol_40_sls
# Extrapolated data
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SimgridStarpuData14.org tmp/simgrid_extrapol_100_e18
./starpu-simgrid/get_trace.sh starpu-simgrid/data/dataExtrapol/SimgridStarpuData15.org tmp/simgrid_extrapol_400_e18
#+end_src

#+RESULTS:


  Extracting makespan for extrapolated e18 and sls matrices:
#+begin_src sh :shebang "#!/bin/bash" :results output :exports none
output="tmp/makespans_extrapol.out"
echo "Matrix, Nthreads, Native Time [ms], SimGrid Time [ms], Diff Time" > $output

i=0
matrices="e18"
nthreads=(1 2 4 5 8 10 40)
datafolder="starpu-simgrid/data/dataExtrapol"
for thread in ${nthreads[@]}
do
   native_time=$(tail -1 $datafolder/SoloStarpuData$i.org)
   simgrid_time=$(tail -1 $datafolder/SimgridStarpuData$i.org)
   diff_time=$(bc -l <<< "(1 - ($simgrid_time / $native_time)) * 100" | sed 's/\(-\?[0-9]*\.[0-9]\?\)[0-9]*/\1/')
   echo "$matrices, $thread,  $native_time, $simgrid_time, $diff_time" >> $output
   i=`expr $i + 1`
done

matrices="sls"
nthreads=(1 2 4 5 8 10 40)
datafolder="starpu-simgrid/data/dataExtrapol"
for thread in ${nthreads[@]}
do
   native_time=$(tail -1 $datafolder/SoloStarpuData$i.org)
   simgrid_time=$(tail -1 $datafolder/SimgridStarpuData$i.org)
   diff_time=$(bc -l <<< "(1 - ($simgrid_time / $native_time)) * 100" | sed 's/\(-\?[0-9]*\.[0-9]\?\)[0-9]*/\1/')
   echo "$matrices, $thread,  $native_time, $simgrid_time, $diff_time" >> $output
   i=`expr $i + 1`
done

matrices="e18"
nthreads=(100 400)
datafolder="starpu-simgrid/data/dataExtrapol"
for thread in ${nthreads[@]}
do
   simgrid_time=$(tail -1 $datafolder/SimgridStarpuData$i.org)
   echo "$matrices, $thread,  0, $simgrid_time, 0" >> $output
   i=`expr $i + 1`
done

matrices="sls"
nthreads=(100 400)
datafolder="starpu-simgrid/data/dataExtrapol"
for thread in ${nthreads[@]}
do
   simgrid_time=$(tail -1 $datafolder/SimgridStarpuData$i.org)
   echo "$matrices, $thread,  0, $simgrid_time, 0" >> $output
   i=`expr $i + 1`
done
#+end_src

#+RESULTS:


* Introduction

  La majorité des supercalculateurs actuels, comme le montre le site
  [[http://www.top500.org][top500]] sont des clusters massivement parallèles et composés de
  noeuds hybrides (CPU-GPU). Pour les programmer, il existe certains
  standards. Il y a tout d'abord la norme MPI (Message Passing
  Interface), qui est une API de communication basée sur l'envoi et la  
  réception de message. Elle a pour objectif d'être performante et
  portable.  Elle est de plus haut niveau que les sockets et apporte
  des mécanismes comme des fonctions de communications collectives
  (exemple broadcast). Ensuite, il y a l'API OpenMP qui est une
  interface de multihreading de plus haut niveau de PThread. Elle
  permet de découper facilement des traitements et d'exploiter les
  architectures multicoeurs. Enfin, il y a l'API CUDA qui permet de
  tirer partie de la puissance de calcul des GPUs. Pour cela il est
  nécessaire de spécifier explicitement de ce que l'on veut envoyer
  aux GPUs et on doit également gérer la synchronisation entre les
  CPUs et les GPUs.
  
  Si l'on veut optimiser le rendement d'une application afin que
  celle-ci tire partie de toute la puissance disponible, il est
  nécessaire d'utiliser plusieurs paradigmes à la fois ce qui
  complique grandement la tâches. Nous sommes donc face à un problème
  de programmation classique où l'on doit, avec les APIs précédentes,
  indiquer explicitement où et quand chacun des calculs doit être 
  réalisé. Par exemple pour exploiter efficacement un GPU, on doit
  transférer explicitement les données du CPU vers le GPU, lancer
  l'exécution du calcul sur le GPU, gérer la synchronisation sur
  l'attente du résultat, récupérer le résultat et pendant ce temps
  continuer à occuper le CPU avec un autre calcul. Cette exemple
  n'illustre que la difficulté liée à la répartition de charge au
  niveau d'un seul et même noeud. Cela ce complique davantage lorsque
  l'on veut également répartir la charge entre plusieurs machines. 
  # Généralement, on procède soit en déléguant tous les
  # calculs aux GPUs, et les CPUs sont en idle. Soit on réparti la
  # charge entre les CPUs et les GPUs de manière complètement
  # statique\cite{StarPU-MPI}. L'inconvénient est que la mise en
  # pratique est très difficile car trouver un bon équilibrage relève
  # d'un véritable travail d'horloger. 
  Cependant même si l'on arrive à équilibrer les charges correctement,
  cette solution est difficilement portable vers une autre machine.  

  La solution serait donc d'avoir une gestion dynamique des
  charges. Mais cela s'avère bien plus compliqué, voir impossible
  à réaliser directement avec ces méthodes de
  programmation. L'alternative est la programmation par tâches. Ce     
  paradigme fournit une abstraction à la notion d'exécution de calculs sur CPU,
  GPU et sur d'autres machines. Ainsi le développeur n'a plus à se
  soucier de sur quelle ressource le calcul est effectué, mais
  seulement d'exprimer le calcul sous la forme d'un graphe de
  tâches. De plus avec un système de répartition dynamique
  l'utilisateur n'aurait également plus de besoin de soucier de quand
  les traitements doivent être effectués. La librairie
  StarPU\cite{StarPU} est un exemple utilisant cette approche, c'est
  cette dernière que nous allons utiliser. C'est un système runtime
  qui permet une répartition des traitements de manière dynamique et
  opportuniste. Pour ce faire, StarPU tient à jour un graphe de dépendance
  permettant d'optimiser l'ordonnancement des tâches. La première
  version de StarPU a été conçu spécialement pour des architectures
  hybrides. Une version récente (StarPU MPI)\cite{StarPU-MPI} a été 
  réalisée pour bénéficier d'un ordonnancement et d'une exécution qui
  soit à la fois dynamique et opportuniste dans un contexte distribuée.
  
  Les performances d'un tel système sont difficiles à évaluer pour
  plusieurs raisons. Tout d'abord, la configuration du runtime
  est un paramètre à prendre en compte, on peut choisir des
  heuristiques et des politiques d'ordonnancements différentes.
  Ensuite, il y a les réglages au niveau de l'application qu'il faut
  prendre en compte, notamment le découpage des tâches, qui entraîne
  la génération d'un graphe de tâches différent.

  Dans cet objectif, la première partie de ce rapport montre qu'une
  des approches possible est la simulation. La seconde partie
  présente en détail le fonctionnement de StarPU et SimGrid 
  ainsi que les difficultés rencontrées. La troisième partie est
  consacrée à la méthodologie employée.  Une quatrième partie montre la
  contribution à la simulation de telles applications.  Et la
  cinquième partie aborde ce que nous avons réussit à mettre en place. 

* État de l'art
  En HPC, il y a deux grandes approches possibles pour évaluer les
  performances d'applications.
** Test sur systèmes réels
   Cette approche consiste à lancer la vrai application sur le système
   réel afin d'effectuer les mesures. Cependant cette méthode peut se 
   révéler très coûteuse et il n'est pas toujours possible d'avoir
   accès à la plateforme. De plus comme les exécutions sont non
   déterministes il est indispensable de réaliser un grand nombre
   d'expériences, or à cause du coût il n'est possible que d'effectuer
   qu'un petit nombre de mesure sur un nombre restreint de
   plate-forme. Il est donc difficile d'extrapoler ce qui serait
   pourtant très utile aux développeurs de runtimes et d'applications. 
** Simulation
   La simulation à pour but de définir un modèle et de calculer une
   prédiction du comportement du système

   Le principe de la simulation est de s'affranchir de la plateforme.
   Ainsi, les expérimentations peuvent être effectuées à partir de
   n'importe quel système, il n'est plus nécessaire d'avoir accès à la
   plateforme, ce qui rend cette approche peu coûteuse. Comme la
   simulation nous permettrait d'avoir un contrôle sur de nombreux
   paramètres, nous pouvons avoir un système déterministe qui 
   nous permettrait d'avoir des expériences qui peuvent être reproduites. 
   Par ailleurs il est facile d'extrapoler les résultats car on peut
   changer les paramètres du modèle. Enfin la simulation
   permet d'avoir un temps d'exécution plus court qu'avec des tests
   réels car on n'effectue que certains traitements ce qui nous permet
   pouvoir effectuer un grand nombre de mesures.  
  
*** L'approche par rejeu de trace
    Cette méthode consiste à exécuter une première fois l'application
    sur un système réel pour ensuite pour ensuite rejouer la trace
    post-mortem. Elle est couramment employée dans le contexte 
    d'applications MPI statiques. Ici, nous avons à faire à une
    exécution complètement dynamique, ce qui est totalement inadaptés car
    le flot de contrôle du programme est non déterministes. 
*** Hybride simulation / émulation
    Ici, nous avons la simulation où l'on crée un faux environnement
    proche de la réalité et où les actions ne sont pas réellement 
    effectués. Dans notre cas on simulerait donc la plateforme de même
    que l'OS. Et en plus on utiliserait l'émulation où l'on exécuterait en
    vrai, mais de manière contrôlée le programme sur le système
    simulé. Cette approche est suivie dans \cite{StarPUSG} où StarPU a
    été porté sur SimGrid mais avec un noeud seulement. Notre objectif
    est d'utiliser la même approche mais avec plusieurs noeuds et avec
    StarPU MPI. 

    L'approche simulation et émulation se révèle donc la plus adaptée.
    Nous avons choisi le simulateur SimGrid qui permet de simuler des
    systèmes distribués, des grilles des calculs, des systèmes peer to
    peer et cloud. De plus StarPU a récemment été porté au-dessus de
    SimGrid et concilie l'approche simulation et évaluation.

* Analyse du problème
** SimGrid
   La structure de SimGrid est composé de plusieurs APIs. Il y a tout
   d'abord l'API SIMIX qui permet de simuler la partie OS. C'est elle
   qui s'occupe notamment de la gestion et de l'ordonnancement des
   processus et également des mécanismes de synchronisation. Sous
   SimGrid, les processus sont modélisés par des threads, ce qui
   signifie que leur espace d'adressage est partagé ce qui nous permet
   de simuler un environnement à mémoire partagée. 
   
   Ensuite, au dessus SIMIX, il y a d'une part l'API MSG. Cette dernière
   permet à l'utilisateur créer et manipuler des processus de manière
   simple. C'est cette API qui est généralement utilisé pour la
   plupart des applications classiques et hybrides. 

   Et d'autre part, il y a l'API SMPI qui a été développée
   spécifiquement pour simuler des applications MPI. Actuellement la
   majeure partie des fonctionnalités de MPI ont été implémentées. La
   simulation de code MPI est assez compliquée et SimGrid est un des
   seul simulateurs à le permettre. Pour ce faire, on compile
   l'application que l'on veut tester en remplaçant le mpi.h classique
   par le mpi.h de SimGrid. Ensuite, à l'édition de liens on remplace
   le main de l'application par le main de SimGrid. Ce dernier a pour
   rôle de préparer l'exécution du simulateur en créant la plateforme
   et en déployant les processus SMPI qui exécuterons chacun le main
   de l'application MPI. Comme dans le cadre d'applications MPI on est
   dans un environnement à mémoire distribuée et que sous SimGrid les
   processus sont modélisés par des threads, afin de simuler le fait
   que chacun ait un espace d'adresse séparé, l'approche suivi par SMPI
   consiste à privatiser les variables des processus en créant pour
   chacun un segment de données virtuel. Pour cela, pour chaque processus
   une nouvelle zone mémoire est créée dans le tas grâce à un mmap, puis le
   segment de données est recopié dans cette zone et à chaque
   changement de contexte on fait pointer vers la zone correspondant à
   celle du processus. 

   #+ATTR_LATEX: :width 5cm
   #+CAPTION: Privatisation du segment données
   #+NAME:   fig:1
   [[./Img/Memoire.pdf]]
   
   Enfin, il y a l'API SURF qui a pour objectif de décrire les
   caractéristique de la plateforme et de la simuler. On lui fournit
   donc une modèle de performance qui permettra d'estimer la durée des
   calculs et des transferts.

** StarPU-MSG: Architecture générale   
   Comme à la base StarPU visait le modèle CPUs-GPUs, l'API la plus
   proche était MSG, notamment par rapport à la création de threads et
   pour la synchronisation. StarPU a donc été modifié pour pouvoir
   fonctionner au dessus du simulateur SimGrid en se basant sur
   MSG. Ainsi, l'application (le runtime de StarPU) est réellement
   exécutée, mais les allocations mémoires des tâches ne sont pas
   effectuées, les codes de calcul sont simulés et remplacés par un
   délais de même pour les transferts CUDA. 

** StarPU-SMPI:Ce qui coince
   Avec StarPU MPI, la modélisation est différente. On est à la fois
   un environnement à mémoire partagée (entre les CPUs et les GPUs
   d'une même machine) et un environnement à mémoire distribuée
   (entre les différents noeuds). On doit donc permettre d'avoir des
   modèles différents selon qu'on est entre noeud où à l'intérieur
   d'un noeud. Il nous faut donc activer la privatisation de variables
   entre les noeuds mais également le partage de variables à
   l'intérieur de chacun noeuds. 

   Pour cela nous avons besoin de faire fonctionner MSG et SMPI
   ensemble. Or non seulement StarPU est essentiellement basé sur MSG
   et de plus MSG et SMPI n'ont pas été prévu pour fonctionner
   ensemble. Il faudra donc initialiser correctement à la fois la
   partie MSG et la partie SMPI. 
   
   Il y a un également un autre point à prendre en considération,
   celui des librairies dynamiques. 
   
   #+ATTR_LATEX: :width 5cm
   #+CAPTION: Emplacement en mémoire des bibliothèques
   #+NAME:   fig:2
   [[./Img/Dyn.pdf]]

   Dans SimGrid seul le segment données est privatisé, comme les
   variables globales des librairies dynamiques ne se trouvent pas
   dans ce dernier(DSO sur le schéma ci-dessus), elles restent donc
   accessible accessibles à tous les processus SimGrid. Nous devrons donc
   également faire en sorte de privatiser les variables globales des
   librairies externes entre les noeuds. 
   
* Méthodologie
  Comme nous travaillons avec SimGrid et StarPU à la fois, nous
  utilisons un dépôt complexe comprenant les deux et géré avec
  l'outils submodule de [[https://github.com/swhatelse/Journal][git]]. Ce dernier nous permet de gérer des sous
  dépôt indépendemment, ainsi il est plus aisé de traiter les mises à
  jours de ces derniers.

  Afin de pouvoir retracer le cheminement de mon travail, mais aussi
  de pouvoir garder le fil d'un jour à l'autre, un cahier de
  laboratoire est tenu en org-mode et est hébergé sur github. Cela permet
  également à mon tuteur de stage de connaître chaque jours l'avancement
  du projet et des difficultés rencontrées.
  
  Comme on l'a vu précédemment il est nécessaire d'apporter quelques
  modifications au niveau du simulateur et de StarPU. Dans ce but, il
  a été dans un premier temps nécessaire de consulter la documentation
  afin de comprendre le fonctionnement et l'architecture de
  SimGrid. Ensuite il a fallut explorer le code afin de déterminer où
  et comment apporter les modifications. Pour cela les outils tels que
  GDB et Valgrind ont été d'une aide précieuse et ont permis de notamment
  vérifier que les changements de segment mémoire s'effectuent bien au
  bon moment.

* Contribution
  La toute première chose à réaliser, a été la gestion du partage du
  segment de données au niveau du simulateur dans un contexte
  SMPI. Comme la mémoire est partagée au sein d'un noeud, nous avons
  fait en sorte que les processus d'un même noeud aient leurs segment
  données en commun. Le principe est le suivant, il y a dans un
  premier temps, les processus SMPI qui sont créés au lancement de
  l'application avec leur propre espace de données. Puis ces derniers
  peuvent à leurs tours créer de nouveau processus. Ceux-ci héritent
  donc du segment de données du processus qui les a créés. Il a par
  ailleurs été nécessaire d'initialiser MSG et SMPI
  correctement afin que les deux puissent fonctionner
  ensemble. SimGrid a donc été modifié en conséquences. 

  Une fois la gestion du partage mise en place, nous nous sommes
  penchés sur le cas des bibliothèques dynamiques. Nous avons vu
  précédemment que malgré le mécanisme de privatisation, les variables
  globales présentes dans ces dernières sont partagés entre les
  différents processus SimGrid. Pour contourner ce problème, nous
  avons décidé d'utiliser une version statique de la bibliothèque.  
  
  #+ATTR_LATEX: :width 5cm
  #+CAPTION: Emplacement en mémoire des bibliothèques
  #+NAME:   fig:3
  [[./Img/StaticDyn.jpg]]

  Ainsi avec une bibliothèque statique, les variables globales de
  celle-ci se retrouvent dans le segment données du processus et la
  gestion du partage / privatisation est géré par le mécanisme
  précédent. Cependant cette solution est relativement intrusive car
  elle nécessite de changer la chaîne de compilation des applications
  utilisant StarPU, mais cela sera suffisants dans un premier temps. 

  Comme StarPU a été porté au dessus de MSG, il a également été
  nécessaire d'apporter quelques modifications au niveau de
  l'initialisation. Car le mécanisme de gestion de la privatisation et
  de partage n'était activée que de manière tardive. 

* Validation
** Test simple
   Dans le but de tester le bon fonctionnement des modifications
   apportées, un test illustrant le fonctionnement de StarPU a été
   fourni et enrichi. Ce dernier permet ainsi d'isoler le problème
   afin de pouvoir nous concentrer dessus. Ce test, initialise SimGrid
   et la partie SMPI comme cela est fait du côté de StarPU et fait
   appel à une bibliothèque dynamique et manipule des variables
   globales. Ainsi lors de l'exécution de ce test, on doit pouvoir
   constater que pour des processus appartenant à un même noeuds, les
   valeurs des variables globales du programme et des bibliothèques
   dynamiques sont bien identiques. Ce qui après plusieurs correction
   a été le cas.  
** Test de StarPU - SMPI
   Comme les résultats du test simples étaient ceux attendu, nous
   sommes passé à un test utilisant cette fois la vrai bibliothèque
   StarPU. Cette dernière est fourni avec des exemples de programme MPI
   notamment d'algèbre linéaire tel que l'algorithme de Cholesky. Nous
   nous sommes servi de ces derniers afin de valider les
   modifications. Cependant au cours des tests nous avons constaté
   qu'un problème persistait au niveau de l'initialisation de StarPU.

* Conclusion
  Pour conclure, nous avons voulu voir s'il était possible de mesurer
  l'influence d'un runtime dynamique sur les performances
  d'applications MPI. Parmi les différentes techniques de mesures de
  performances, nous avons fait le choix de la simulation / émulation
  car elle nous semble la plus avantageuse, en raison de son coût, de
  sa flexibilité mais aussi en terme de scalabilité.  
  
  Pour vérifier si cette approche est effectivement possible, nous
  avons modifié SimGrid afin de pouvoir faire fonctionner StarPU MPI
  dessus. Nous avons donc mis en place le partage du segment données
  entre les processus de même noeud et la privatisation entre les
  processus de noeuds différents. 
  
  Bien qu'aucune expérimentation n'est pu être faite, les problèmes
  rencontrés sont plutôt des problèmes d'ordre techniques et ne nous
  permettent pas d'invalider notre hypothèse. Afin de pouvoir conclure
  sur la question, il faudra finir de corriger la phase
  d'initialisation côté StarPU et également apporter quelques
  correctifs à SimGrid. Ensuite nous pourrons effectuer les
  simulations et les mesures. Pour ce faire les mesures seront faites
  sur un solveur d'algèbre linéaire basé sur StarPU. Enfin, dans le
  but de valider le résultat des expérimentations, un test grandeur
  nature sera fait sur Grid5000.  
  
#+Latex:\section*{Acknowledgments}
Je souhaite remercier...

#+LaTeX: \nocite{*}
#+LaTeX: \def\raggedright{}
\bibliographystyle{IEEEtran}
\bibliography{biblio}


* Emacs Setup 							   :noexport:
  This document has local variables in its postembule, which should
  allow Org-mode to work seamlessly without any setup. If you're
  uncomfortable using such variables, you can safely ignore them at
  startup. Exporting may require that you copy them in your .emacs.

# Local Variables:
# eval:    (require 'org-install)
# eval:    (org-babel-do-load-languages 'org-babel-load-languages '( (sh . t) (R . t) (perl . t) (ditaa . t) ))
# eval:    (setq org-confirm-babel-evaluate nil)
# eval:    (unless (boundp 'org-latex-classes) (setq org-latex-classes nil))
# eval:    (add-to-list 'org-latex-classes '("svjour3" "\\documentclass[smallextended]{svjour3} \n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n  \\usepackage{graphicx}\n  \\usepackage{hyperref}"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (add-to-list 'org-latex-classes '("acm-proc-article-sp" "\\documentclass{acm_proc_article-sp}\n \[NO-DEFAULT-PACKAGES]\n \[EXTRA]\n"  ("\\section{%s}" . "\\section*{%s}") ("\\subsection{%s}" . "\\subsection*{%s}")                       ("\\subsubsection{%s}" . "\\subsubsection*{%s}")                       ("\\paragraph{%s}" . "\\paragraph*{%s}")                       ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
# eval:    (setq org-alphabetical-lists t)
# eval:    (setq org-src-fontify-natively t)
# eval:   (setq org-export-babel-evaluate nil)
# eval:   (setq ispell-local-dictionary "french")
# eval:   (eval (flyspell-mode t))
# eval:    (setq org-latex-listings 'minted)
# eval:    (setq org-latex-minted-options '(("bgcolor" "white") ("style" "tango") ("numbers" "left") ("numbersep" "5pt")))
# End:
