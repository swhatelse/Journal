* Intro -> Abstract
** Draft
  Supercalculateur -> course à la performance.
  Mais facteurs limitant coût energétique, chauffe.
  Envisager changement de methode de programmation visant à optimiser
  l'utilisation des ressources.
  Production nucléaire central nucléaire : 900 - 1450 MW
  source:
  http://energie.edf.com/nucleaire/comment-ca-marche-y/les-differents-types-de-centrales-nucleaires-48384.html

** Pas draft
  Dans le domaine des supercalculateurs, la course à la performance est
  un point crucial. Actuellement, le supercalculateur le plus puissant (le
  TianHe-2) est capable d'effectuer environ 33.86 Peta opérations
  flotantes par secondes. Cependant, cette course est freinée par un
  facteur qui prend désormais une importance capitale: le coût
  énergétique. En effet, le supercalculateur chinois TianHe-2 a une
  consommation qui atteint presque les 18MW et
  avec la génération exascale, la consommation estimée sera entre 20MW
  et 40MW. Dans l'état des faits, ce n'est pas réalisable et pour
  pouvoir atteindre l'exaflops, il est nécessaire d'optimiser d'autres
  points que la puissance des puces. Evidemment des optimisations
  peuvent être faites au niveau matériel afin de réaliser des
  composants à haute efficacité énergétique. On peut également
  optimiser le rendement en utilisant au mieux les capacités du
  matériel. Cette optimisation se fait donc du côté logiciel et pour
  cela il nous faut  envisager un changement de méthode de programmation.
  C'est cette dernière approche que nous allons étudier. L'objectif de mon
  stage au sein de l'équipe MESCAL, sous la tutelle d'Arnaud Legrand,
  est donc de tenter de mesurer le gain d'une telle solution. 
  
  Pour cela nous allons, dans une première partie, voir comment est
  effectuée en générale la programmation en HPC (High Performance computing), quels sont les différents
  standards et pourquoi nous nous sommes concentrés sur MPI. Nous
  discuterons ensuite du principe et de l'intérêt d'un nouveau
  paradigme de programmation et de la librairie StarPU. Nous
  constaterons ensuite que malgrès les apports de cette méthodes des
  difficultés subsites et les mesures peuvent-être compliquées a
  effectuées. C'est pourquoi dans une seconde partie, nous étudierons
  les différents approches pour évaluer les performances
  d'applications HPC et nous justifierons notre choix pour la
  simulation/émulation et en particulier pour l'outils Simgrid. Dans
  une troisième partie nous examinerons en détail Simgrid et StarPU
  ainsi que les différents problèmes que nous avons rencontrés. Dans
  une quatrième partie, nous verrons les méthodes employées. En
  cinquième partie, nous verrons les modifications apportés à Simgrid
  afin de pouvoir effectuer les mesures. Ensuite dans une sixième
  partie, nous verrons comment ces changements ont été validés. Et
  pout finir nous conclurons sur les résultats que nous avons réussit
  à obtenir.

* Contexte / Problèmatique -> Real Intro 1
** Programmation "classique": MPI, OpenMP, CUDA
*** Draft
    source: http://www.top500.org
    Système à mémoire distribuée. Cluster, hétérogène.

    De facto Standard en HPC:
   - MPI: API de "haut niveau" basé sur l'envoi et la récéption de
     message.
     
   - OpenMP: Plus haut niveau que pthread, demande d'annoter les
     boucles, pas de possibilité de gérer des différences de priorités
     entre des threads de calcul et des threads de polling réseau par
     exemple.
     
   - CUDA: Déléguer des calculs aux GPUs. Demande de transférer
     explicitement les données et de faire la synchronisation.
     
   Compliqué à programmer, à avoir des performances portables:
   - granularité = difficile d'être portable
   - équilibrage de charge super difficile à faire statiquement,
     le dynamique est super pénible (impossible) directement avec ces
     modèles de programmation.
   - même si pas de déséquilibrage de charge, une exécution
     statique/rigide induit artificiellement de l'idle time.
*** Pas draft
     La majorité des supercalculateurs actuels sont des clusters
     massivement parallèles et souvent de type
     hétérogènes(CPU-GPU). De ce fait certains standard ce sont
     imposés. 

     Il y a tout d'abord la norme MPI (Message Passing Interface),
     qui est une API de communication basée sur l'envoi et la
     récéption de message. Elle réputée pour être performante et
     portable, et elle est de plus haut niveau que les sockets.
     
     Ensuite, il y a l'API OpenMP qui est une interface de
     multihreading de plus haut niveau de PThread...

     Enfin, l'API CUDA permet tirer partie de la puissance de calculer
     des GPU. Mais pour cela il est nécessaire de spécifier
     explicitement de ce que l'on veut envoyer aux GPUs et on doit
     également gérer la synchronisation 

     L'objectif étant optimiser le rendemment d'une application afin
     que cette dernière tire partie de toute la puissance disponible, il faut
     faire en sorte d'occuper au maximum le plus d'unités de calcul possible.
     Le problème est que l'on se retrouve à devoir utiliser plusieurs
     paradigme à la fois ce qui complique grandement la
     programmation.

     Généralement, on procède soit en délèguant tout 
     les calculs aux GPUs, laissant les CPUs en idle. Soit on réparti
     la charge entre les CPUs et les GPUs de manière complètement
     statique. L'inconvéniant est que la mise en pratique très
     difficile car il est hardu de trouver un bon équilibrage. 
     
     Cependant même si l'on arrive à équilibrer les charges
     correctemment, on peut avoir des cas où certaines unités de
     caluls ne sont pas occupées alors qu'elles le pourraient. Cela se
     produit quand par exemple lorsque certaines unités de calculs
     attendent la terminaisons de certain traitements alors que
     d'autres auraient put être effectuer en attendant. Cela est dû au
     fait que l'exécution soit statique et induit un idle time
     artificiel. De plus cette solution n'est pas portable car le
     découpage des traitements ce fait en fonction de la plateforme
     cible.

     La solution serait donc d'avoir une gestion dynamique des
     charges. Mais cela s'avère bien plus ardue, voir impossible
     à réaliser directement avec ces méthodes de programmation. Alors
     essayons en une autre.
     
** Programmation "nouvelle génération": StarPU
*** Draft
   L'architecture des HPC est une architecture massivement parallèle
   et le rendement peut-être améliorer en occupant le plus possible un
   maximum d'unité de calcul.

   Système runtime qui permet la répartition de charge de travail.
     - Paradigme -> tâches.
     - Génération d'un graphe de dépendance -> optimiser ordonnancement des tâches.
   Première version spécialement conçue pour architectures hybrides.

   Version récente (StarPU MPI) pour bénéficier de
   l'exécution/ordonnancement dynamique/opportuniste aussi dans le cas
   du distribué et potentiellement répartir dynamiquement les tâches
   en fonction de la charge entre les noeuds.
*** Pas draft
    La librairie StarPU est un système runtime qui permet une
    répartition des traitements de manière dynamique et opportuniste. 
    Pour ce faire elle introduit un nouveau paradigme basé sur les
    tâches. StarPU génère un graphe de dépendance permettant
    d'optimiser le l'ordonnancement de ces dernières.

    La première version de StarPU a été conçu spécialement pour des
    architectures hybrides. Une version récente (StarPU MPI) a été
    réalisée pour bénéficier d'un ordonnance et d'une exécution qui
    soit à la fois dynamique et opportunistes dans un contexte
    distribuée afin de répartir la charge entre les différents
    noeuds.

** Des difficultés récurrentes:
*** Draft
   - Configuration du runtime
   - Exécution encore moins déterministe qu'avant donc difficile à
     modéliser. Difficulté de dimensionnement.
   - Impact de la plate-forme.
*** Pas draft
    Cependant des difficultés persitent. Comme l'exécution est
    dynamique et opportuniste, nous sommes très loin d'un modèle
    déterministe ce qui rend la modélisation difficile.
    
** Problèmatique:
*** Draft
    Comment évaluer les performances d'applications StarPU MPI?
*** Pas draft
    Nous allons donc voir comment évaluer les performances
    d'applications basés sur StarPU MPI.
* Etat de l'art 1
*** Draft
    Approches pour l'évaluation de performance d'applications HPC

    - Caractéristiques:
      Lever les ambiguïtés
      Quels sont les autres approches possibles?
      Pourquoi on suit cette piste?
      Expliquer les choix
*** Pas draft
  En HPC, il y a trois grandes approches possible pour évaluer les
  performances d'application.
** Test sur systèmes réels
*** Draft
   Difficulté d'accès à la plateforme, reproductibilité des
   expériences, coût, extrapolation, ...

   On voudrait une approche moins coûteuse...
*** Pas draft
    La première est de lancer la vrai application sur le système
    réelle et d'effectuer les mesures. Cependant cette méthode peux se
    révéler très couteuse.
** L'approche par rejeu de trace
*** Draft
   Classique pour étudier des applications MPI mais inadaptée ici car
   exécution dynamique.
*** Pas draft
** La simulation/émulation
*** Draft
   L'application et le runtime sont exécutés pour de vrai (émulés)
   mais l'exécution des kernels de calculs et les transferts de
   données sont simulés. 

   C'est l'approche suivie dans StarPU/Simgrid et SMPI
   
   À la base un Simulateur de systèmes distribués et de grilles de
  calculs, systèmes peer to peer, cloud.

  Récement étendu pour gérer les applications MPI et récemment
  développement d'un mode "simulation" pour StarPU.

  Les deux approches existe mais arrive-t-on à les utiliser
  ensemble?...
  
*** Pas draft
* Analyse du problème 1
** SG:
*** Draft
    Plusieurs API MSG, SMPI, un seul kernel SIMIX
** SimGrid/MPI: Architecture générale
*** Draft
   Principe: smpi fourni l'API MPI. À la compilation (smpicc) compile
   avec un mpi.h de SimGrid (compatible avec la majeur partie du
   standard MPI) remplace le main (avec cpp) par smpi_simulated_main et
   link avec la libsimgrid qui fournit son propre main (en weak).

   Le smpirun prépare l'exécution du simulateur (platform, deploiement
   des process) et appelle le main de smpi qui créée des threads qui
   appellement le smpi_simulated_main.

   Si appli avec openMP ou pthread ou CUDA, rien n'est intercepté et
   ça va faire n'importe quoi. 

   Le code de l'application est exécuté pour de vrai mais les
   communications passent par MPI et sont donc simulées. À chaque
   appel MPI, il y a un changement de contexte qui rend la main au
   simulateur et qui permet de décider quel thread on débloque.

   Attention, Processus modélisé par threads, donc espace d'adressage
   partagé et donc exécution complètement incorrecte... L'approche
   suivie par SMPI consiste à privatiser les variables des processus
   en mmapant le segment data.

   Émulation automatique et complète donc a priori très lent mais
   possibilité d'annoter le code pour:
   - diminuer le temps d'exécution: ne pas exécuter certaines portions
     de code mais insérer à la place un délai simulé.
   - diminuer l'empreinte mémoire: ne pas allouer toutes les données
     (ne pas allouer ou bien faire de l'aliasing mémoire).
** StarPU-SG: Architecture générale
*** Draft
   Basé sur MSG car API la plus proche (en particulier, création de
   threads et de synchros).
   
   Application exécutée pour de vrai. StarPU a été modifié de façon à:
   - ne pas faire les allocations mémoires des tâches
   - ne pas exécuter les codes de calcul des tâches mais insérer un
     délai simulé à la place
   - ne pas faire de transferts CUDA (car la machine sur la quelle on
     fait la simulation peut même ne pas avoir de GPU du tout) mais
     faire des transferts simulés à la place

   StarPU de base = des threads dans un seul processus donc rien
   d'aussi compliqué à faire que ce qui avait été nécessaire pour
   SMPI.

** Ce qui coince
*** Draft
  De base, MSG et SMPI pas prévus pour fonctionner ensemble. Besoin de
  - partage de data par les threads StarPU appartenant au même
    processus MPI. Attention aux librairies dynamiques.
  - Initialiser correctement à la fois la partie SMPI de SimGrid et la
    partie MSG
  - Permettre d'avoir des modèles différents selon qu'on est entre
    noeuds ou à l'intérieur d'un noeud
* Méthodologie 1
*** Draft
  - Modification de deux code bases complexes. Utilisation de git
    submodule comprenant les deux.
  - Utilisation d'org-mode/github pour cahier de laboratoire.
  - Utilisation de valgrind, gdb, emacs/etags/cgvg pour exploration du
    code et déterminer où apporter les modifications
  - Développements disponibles dans git et bientôt intégrés à SimGrid
    et à StarPU.
* Contribution 2 / 3
*** Draft
  - J'ai géré le partage du segment data en rajoutant ce qu'il fallait
    au niveau du changement de contexte (un indice par processus
    MPI...). Tout processus créé par MSG hérite du segment data de son
    père alors que les processus créés par MPI dupliquent le segment
    data de leur créateur.
  - Pour les bibliothèques dynamiques, on a simplement linké
    statiquement celles qui doivent l'être. C'est une limitation car
    ça demande de changer la chaîne de compilation des applications
    utilisant starPU mais ça ira bien pour commencer.
  - La double initialisation de MSG et de SMPI n'a pas posé de
    problème car elles étaient déjà préparées à celà. Seule difficulté
    à laquelle on n'a pas répondu: actuellement, on initialise MSG
    pour toutes les applications MPI, ce qui induit dans le cas
    général un overhead mémoire. On n'aimerait ne faire
    l'initialisation de MSG que dans le cas où on exécute StarPU/MPI.
    - problème du weak main et de rajouter un MSG_init dans cette
      chaine d'éxécution
  - Utilisation de modèles de performance différents pour inter et
    intra noeuds:
    - Ça demande des modifications complexes dans les couches basses
      de SimGrid (surf) et on n'a pas regardé pour l'instant.
      
* Validation 2
  - Caractéristiques:
    Resultat expérience + interprétation
** Test d'un cas simplifié d illustrant le comportement recherché
** Test starpu smpi
* Conclusion 
  - Caractéristiques
    Conséquences:
    Travaux futurs:
    -Test grid5k
    
