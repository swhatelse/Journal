* Intro -> Abstract
** Draft
  Supercalculateur -> course à la performance.
  Mais facteurs limitant coût energétique, chauffe.
  Envisager changement de methode de programmation visant à optimiser
  l'utilisation des ressources.
  Production nucléaire central nucléaire : 900 - 1450 MW
  source:
  http://energie.edf.com/nucleaire/comment-ca-marche-y/les-differents-types-de-centrales-nucleaires-48384.html

* Contexte / Problèmatique -> Real Intro 1
** Programmation "classique": MPI, OpenMP, CUDA
   source: http://www.top500.org
    Système à mémoire distribuée. Cluster, hétérogène.

    De facto Standard en HPC:
   - MPI: API de "haut niveau" basé sur l'envoi et la récéption de
     message.
     
   - OpenMP: Plus haut niveau que pthread, demande d'annoter les
     boucles, pas de possibilité de gérer des différences de priorités
     entre des threads de calcul et des threads de polling réseau par
     exemple.
     
   - CUDA: Déléguer des calculs aux GPUs. Demande de transférer
     explicitement les données et de faire la synchronisation.
     
   Compliqué à programmer, à avoir des performances portables:
   - granularité = difficile d'être portable
   - équilibrage de charge super difficile à faire statiquement,
     le dynamique est super pénible (impossible) directement avec ces
     modèles de programmation.
   - même si pas de déséquilibrage de charge, une exécution
     statique/rigide induit artificiellement de l'idle time.
** Programmation "nouvelle génération": StarPU
   L'architecture des HPC est une architecture massivement parallèle
   et le rendement peut-être améliorer en occupant le plus possible un
   maximum d'unité de calcul.

   Système runtime qui permet la répartition de charge de travail.
     - Paradigme -> tâches.
     - Génération d'un graphe de dépendance -> optimiser ordonnancement des tâches.
   Première version spécialement conçue pour architectures hybrides.

   Version récente (StarPU MPI) pour bénéficier de
   l'exécution/ordonnancement dynamique/opportuniste aussi dans le cas
   du distribué et potentiellement répartir dynamiquement les tâches
   en fonction de la charge entre les noeuds.
** Des difficultés récurrentes:
   - Configuration du runtime
   - Exécution encore moins déterministe qu'avant donc difficile à
     modéliser. Difficulté de dimensionnement.
   - Impact de la plate-forme.
** Problèmatique:
    Comment évaluer les performances d'applications StarPU MPI?
* Etat de l'art 1
*** Draft
    Approches pour l'évaluation de performance d'applications HPC

    - Caractéristiques:
      Lever les ambiguïtés
      Quels sont les autres approches possibles?
      Pourquoi on suit cette piste?
      Expliquer les choix
*** Pas draft
    En HPC, il y a trois grandes approches possible pour évaluer les
    performances d'application.
** Test sur systèmes réels
*** Draft
   Difficulté d'accès à la plateforme, reproductibilité des
   expériences, coût, extrapolation, ...

   On voudrait une approche moins coûteuse...
*** Pas draft
    La première est de lancer la vrai application sur le système
    réel afin d'effectuer les mesures. Cependant cette méthode peut se 
    révéler très couteuse et il n'est pas toujours possible d'avoir
    accès à la plateforme. De plus comme les expérimentations ne
    peuvent être effectuées sur que sur un petit nombre de plateforme
    notamment à cause de coût, on ne peut pas vraiment extrapoler les
    résultats. Dernier point important, nous n'avons pas de contrôle
    sur les décisions d'ordonnancements, d'une exécution à l'autre on
    peut avoir des résultats différents ce qui fait que les
    expériences ne sont pas reproductibles. 
** L'approche par rejeu de trace
*** Draft
   Classique pour étudier des applications MPI mais inadaptée ici car
   exécution dynamique.
*** Pas draft
    Cette méthode consiste à exécuter une première l'application sur
    un système réel pour ensuite pour ensuite rejouer la trace
    post-mortem. Elle est couramment employé dans le contexte
    d'application MPI mais est ici totalement inadaptés car nous avons
    à faire des programmes qui sont non déterministes. En effet, on ne
    pourra pas connaitre les autres actions qu'il était possible
    d'effectuer plutôt qu'un autre, ni leur impact.
** La simulation/émulation
*** Draft
   L'application et le runtime sont exécutés pour de vrai (émulés)
   mais l'exécution des kernels de calculs et les transferts de
   données sont simulés. 

   C'est l'approche suivie dans StarPU/Simgrid et SMPI
   
   À la base un Simulateur de systèmes distribués et de grilles de
   calculs, systèmes peer to peer, cloud.

  Récement étendu pour gérer les applications MPI et récemment
  développement d'un mode "simulation" pour StarPU.

  Les deux approches existe mais arrive-t-on à les utiliser
  ensemble?...
  
*** Pas draft
    Ici le runtime et l'application sont émulés (exécutés réellement)
    mais l'exécution des kernels de calculs et les transferts de
    données sont simulés. 
    
    On a d'une part la simulation où l'on crée un faux environnement
    proche de la réalité et où les actions ne sont pas réellement
    effectués. Dans notre cas on simulerait donc la plateforme de même que l'OS. 
    Ainsi, les expérimentations peuvent être effectuées à
    partir de n'importe quel système, il n'est plus nécessaire d'avoir
    accès à la plateforme, ce qui rend cette approche peu
    coûteuse. 
    Par ailleur il est facile d'extrapoler les résultats car
    on peut simuler un nombre important de plateformes.  
    Ensuite la simulation permet d'avoir un temps d'exécution plus
    court qu'avec des tests réels car on n'effectue que certains 
    traitements ce qui nous permet pouvoir effectuer un grand nombre
    de mesures.  
    Enfin comme la simulation nous permettrait d'avoir un contrôle sur
    l'ordonnancement, nous pourrions avoir un système déterministe qui
    nous permettrait d'avoir des expériences qui peuvent être reproduites.
    
    Et d'autre part l'émulation où l'on exécuterait en vrai le programme
    programme sur le système simulé. 
    
    Nous allons tenter de voir si nous pouvons conciller les deux approches.

    Pour cela nous allons utiliser le logiciel Simgrid qui est un simulateur
    de systèmes distribués, de grilles de calculs, de systèmes peer to
    peer et cloud.

* Analyse du problème 1
** SG:
    Plusieurs API MSG, SMPI, un seul kernel SIMIX
** SimGrid/MPI: Architecture générale
   Principe: smpi fourni l'API MPI. À la compilation (smpicc) compile
   avec un mpi.h de SimGrid (compatible avec la majeur partie du
   standard MPI) remplace le main (avec cpp) par smpi_simulated_main et
   link avec la libsimgrid qui fournit son propre main (en weak).

   Le smpirun prépare l'exécution du simulateur (platform, deploiement
   des process) et appelle le main de smpi qui créée des threads qui
   appellement le smpi_simulated_main.

   Si appli avec openMP ou pthread ou CUDA, rien n'est intercepté et
   ça va faire n'importe quoi. 

   Le code de l'application est exécuté pour de vrai mais les
   communications passent par MPI et sont donc simulées. À chaque
   appel MPI, il y a un changement de contexte qui rend la main au
   simulateur et qui permet de décider quel thread on débloque.

   Attention, Processus modélisé par threads, donc espace d'adressage
   partagé et donc exécution complètement incorrecte... L'approche
   suivie par SMPI consiste à privatiser les variables des processus
   en mmapant le segment data.

   Émulation automatique et complète donc a priori très lent mais
   possibilité d'annoter le code pour:
   - diminuer le temps d'exécution: ne pas exécuter certaines portions
     de code mais insérer à la place un délai simulé.
   - diminuer l'empreinte mémoire: ne pas allouer toutes les données
     (ne pas allouer ou bien faire de l'aliasing mémoire).
** StarPU-SG: Architecture générale
   Basé sur MSG car API la plus proche (en particulier, création de
   threads et de synchros).
   
   Application exécutée pour de vrai. StarPU a été modifié de façon à:
   - ne pas faire les allocations mémoires des tâches
   - ne pas exécuter les codes de calcul des tâches mais insérer un
     délai simulé à la place
   - ne pas faire de transferts CUDA (car la machine sur la quelle on
     fait la simulation peut même ne pas avoir de GPU du tout) mais
     faire des transferts simulés à la place

   StarPU de base = des threads dans un seul processus donc rien
   d'aussi compliqué à faire que ce qui avait été nécessaire pour
   SMPI.
** Ce qui coince
   De base, MSG et SMPI pas prévus pour fonctionner ensemble. Besoin de
  - partage de data par les threads StarPU appartenant au même
    processus MPI. Attention aux librairies dynamiques.
  - Initialiser correctement à la fois la partie SMPI de SimGrid et la
    partie MSG
  - Permettre d'avoir des modèles différents selon qu'on est entre
    noeuds ou à l'intérieur d'un noeud
* Méthodologie 1
  - Modification de deux code bases complexes. Utilisation de git
    submodule comprenant les deux.
  - Utilisation d'org-mode/github pour cahier de laboratoire.
  - Utilisation de valgrind, gdb, emacs/etags/cgvg pour exploration du
    code et déterminer où apporter les modifications
  - Développements disponibles dans git et bientôt intégrés à SimGrid
    et à StarPU.
* Contribution 2 / 3
  - J'ai géré le partage du segment data en rajoutant ce qu'il fallait
    au niveau du changement de contexte (un indice par processus
    MPI...). Tout processus créé par MSG hérite du segment data de son
    père alors que les processus créés par MPI dupliquent le segment
    data de leur créateur.
  - Pour les bibliothèques dynamiques, on a simplement linké
    statiquement celles qui doivent l'être. C'est une limitation car
    ça demande de changer la chaîne de compilation des applications
    utilisant starPU mais ça ira bien pour commencer.
  - La double initialisation de MSG et de SMPI n'a pas posé de
    problème car elles étaient déjà préparées à celà. Seule difficulté
    à laquelle on n'a pas répondu: actuellement, on initialise MSG
    pour toutes les applications MPI, ce qui induit dans le cas
    général un overhead mémoire. On n'aimerait ne faire
    l'initialisation de MSG que dans le cas où on exécute StarPU/MPI.
    - problème du weak main et de rajouter un MSG_init dans cette
      chaine d'éxécution
  - Utilisation de modèles de performance différents pour inter et
    intra noeuds:
    - Ça demande des modifications complexes dans les couches basses
      de SimGrid (surf) et on n'a pas regardé pour l'instant.
      
* Validation 2
  - Caractéristiques:
    Resultat expérience + interprétation
** Test d'un cas simplifié d illustrant le comportement recherché
** Test starpu smpi
* Conclusion 
  - Caractéristiques
    Conséquences:
    Travaux futurs:
    -Test grid5k
    

